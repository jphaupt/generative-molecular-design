{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.5.0+cu124\n",
      "PyG version 2.6.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import mygenai\n",
    "from mygenai.models.graphvae import GraphVAE\n",
    "from mygenai.utils.transforms import CompleteGraph, SetTarget, PadToFixedSize, ExtractFeatures\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch version {}\".format(torch.__version__))\n",
    "print(\"PyG version {}\".format(torch_geometric.__version__))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.858491897583008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jph/dev/generative-molecular-design/.conda/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Transforms which are applied during data loading:\n",
    "# (1) Fully connect the graphs, (2) Select the target/label\n",
    "\n",
    "transform = torch_geometric.transforms.Compose([\n",
    "        ExtractFeatures(),\n",
    "        PadToFixedSize(),\n",
    "        CompleteGraph(),\n",
    "        SetTarget()\n",
    "    ])\n",
    "target = 4\n",
    "\n",
    "# Load the QM9 dataset with the transforms defined\n",
    "dataset = QM9(\"../data/QM9\", transform=transform)\n",
    "\n",
    "# Normalize targets per data sample to mean = 0 and std = 1.\n",
    "mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
    "std = dataset.data.y.std(dim=0, keepdim=True)\n",
    "dataset.data.y = (dataset.data.y - mean) / std\n",
    "mean, std = mean[:, target].item(), std[:, target].item()\n",
    "# dataset = dataset[1000]\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 130831.\n",
      "Created dataset splits with 1000 training, 1000 validation, 1000 test samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of samples: {len(dataset)}.\")\n",
    "\n",
    "# let's just use the first 3000 samples\n",
    "\n",
    "# Split datasets (our 3K subset)\n",
    "train_dataset = dataset[:1000]\n",
    "val_dataset = dataset[1000:2000]\n",
    "test_dataset = dataset[2000:3000]\n",
    "print(f\"Created dataset splits with {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples.\")\n",
    "\n",
    "# Create dataloaders with batch size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw water molecule edges:\n",
      "Atom 0 - Atom 1: single\n",
      "Atom 0 - Atom 2: single\n",
      "Atom 1 - Atom 0: single\n",
      "Atom 1 - Atom 2: no bond\n",
      "Atom 2 - Atom 0: single\n",
      "Atom 2 - Atom 1: no bond\n",
      "Dense adjacency for H1-H2 connection:\n",
      "tensor([0., 0., 0., 0., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water = dataset[2]\n",
    "\n",
    "BOND_TYPES = {\n",
    "    0: \"single\",\n",
    "    1: \"double\",\n",
    "    2: \"triple\",\n",
    "    3: \"aromatic\",\n",
    "    4: \"no bond\"\n",
    "}\n",
    "\n",
    "water = water.to(device)\n",
    "# print(\"Water molecule edge attributes: \", water.edge_attr)\n",
    "# print(\"Water molecule node attributes: \", water.x)\n",
    "# print(\"Water molecule edge indices: \", water.edge_index)\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "# create ground-truth adjacency matrix\n",
    "adj = to_dense_adj(water.edge_index, batch=water.batch, edge_attr=water.edge_attr)\n",
    "# print(\"Ground-truth adjacency matrix: \", adj)\n",
    "# water.x\n",
    "water.edge_attr\n",
    "\n",
    "# check the raw edge_index and edge_attr for water\n",
    "# for some reason H1 and H2 are connected with a single bond...\n",
    "print(\"Raw water molecule edges:\")\n",
    "for i in range(water.edge_index.shape[1]):\n",
    "    src = water.edge_index[0, i].item()\n",
    "    dst = water.edge_index[1, i].item()\n",
    "    attr = water.edge_attr[i].argmax().item()\n",
    "    bond_type = BOND_TYPES[attr]\n",
    "    if src < 3 and dst < 3:  # Only real atoms\n",
    "        print(f\"Atom {src} - Atom {dst}: {bond_type}\")\n",
    "\n",
    "# Look at dense adjacency matrix directly\n",
    "dense_adj = to_dense_adj(water.edge_index, batch=water.batch, edge_attr=water.edge_attr)[0]\n",
    "print(\"Dense adjacency for H1-H2 connection:\")\n",
    "print(dense_adj[1, 2])  # Should be [0,0,0,0,1] for \"no bond\"\n",
    "\n",
    "water.num_real_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# test forward passs\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(batch)\n",
    "print(\"Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Train Loss: 0.5110 | Val Loss: 0.2155\n",
      "Epoch 001 | Train Loss: 0.1824 | Val Loss: 0.1464\n",
      "Epoch 002 | Train Loss: 0.1570 | Val Loss: 0.1345\n",
      "Epoch 003 | Train Loss: 0.1393 | Val Loss: 0.1220\n",
      "Epoch 004 | Train Loss: 0.1240 | Val Loss: 0.1195\n",
      "Epoch 005 | Train Loss: 0.1205 | Val Loss: 0.1177\n",
      "Epoch 006 | Train Loss: 0.1189 | Val Loss: 0.1183\n",
      "Epoch 007 | Train Loss: 0.1186 | Val Loss: 0.1156\n",
      "Epoch 008 | Train Loss: 0.1184 | Val Loss: 0.1169\n",
      "Epoch 009 | Train Loss: 0.1187 | Val Loss: 0.1172\n",
      "Epoch 010 | Train Loss: 0.1176 | Val Loss: 0.1208\n",
      "Epoch 011 | Train Loss: 0.1171 | Val Loss: 0.1181\n",
      "Epoch 012 | Train Loss: 0.1168 | Val Loss: 0.1183\n",
      "Epoch 013 | Train Loss: 0.1172 | Val Loss: 0.1184\n",
      "Epoch 014 | Train Loss: 0.1177 | Val Loss: 0.1219\n",
      "Epoch 015 | Train Loss: 0.1172 | Val Loss: 0.1189\n",
      "Epoch 016 | Train Loss: 0.1178 | Val Loss: 0.1198\n",
      "Epoch 017 | Train Loss: 0.1172 | Val Loss: 0.1180\n",
      "Early stopping triggered at epoch 18\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "import mygenai.training.training as training\n",
    "training.train_model(model, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for edge between atoms 0 and 1:\n",
      "tensor([8.5438e-01, 7.7687e-02, 5.1896e-02, 2.7549e-04, 1.5765e-02])\n",
      "Most likely bond type: 0\n",
      "Confidence: 0.8544\n",
      "Probabilities for edge between (padding) atoms 5 and 10:\n",
      "tensor([0., 0., 0., 0., 1.])\n",
      "Probabilities for edge between (padding) atom 8 and (real) atom 1:\n",
      "tensor([0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# feed water to the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(water)\n",
    "edge_attr_logits, mu, logvar, property_pred = outputs\n",
    "\n",
    "edge_probs = F.softmax(edge_attr_logits, dim=-1)\n",
    "\n",
    "# Now you can look at the probabilities for each edge\n",
    "print(\"Probabilities for edge between atoms 0 and 1:\")\n",
    "print(edge_probs[0, 0, 1])  # First batch item, edge from atom 0 to 1\n",
    "\n",
    "# Get the most likely bond type\n",
    "most_likely_type = edge_probs[0, 0, 1].argmax().item()\n",
    "print(f\"Most likely bond type: {most_likely_type}\")\n",
    "\n",
    "# Print confidence in prediction\n",
    "confidence = edge_probs[0, 0, 1, most_likely_type].item()\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "print(\"Probabilities for edge between (padding) atoms 5 and 10:\")\n",
    "print(edge_probs[0, 5, 10])\n",
    "\n",
    "print(\"Probabilities for edge between (padding) atom 8 and (real) atom 1:\")\n",
    "print(edge_probs[0, 8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first check that each of the \"no atom\" nodes have \"no bond\" edges\n",
    "# first three nodes are O, H, H\n",
    "for i in range(3, water.x.shape[0]):\n",
    "    for j in range(water.x.shape[0]):\n",
    "        # should always be [0., 0., 0., 0., 0., 1.]\n",
    "        most_likely_type = edge_probs[0, i, j].argmax().item()\n",
    "        if most_likely_type != 4: # no bond\n",
    "            print(f\"Unexpected bond type ({i}-{j}): {BOND_TYPES[most_likely_type]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bond type between O and H1: single\n",
      "Bond type between O and H2: no bond\n",
      "Bond type between H1 and O: single\n",
      "Bond type between H2 and O: no bond\n",
      "Bond type between H1 and H2: single\n"
     ]
    }
   ],
   "source": [
    "# check single bonds between O and each H\n",
    "o_to_h_1 = edge_probs[0, 0, 1].argmax().item()\n",
    "o_to_h_2 = edge_probs[0, 0, 2].argmax().item()\n",
    "h_to_o_1 = edge_probs[0, 1, 0].argmax().item()\n",
    "h_to_o_2 = edge_probs[0, 2, 0].argmax().item()\n",
    "print(f\"Bond type between O and H1: {BOND_TYPES[o_to_h_1]}\")\n",
    "print(f\"Bond type between O and H2: {BOND_TYPES[o_to_h_2]}\")\n",
    "print(f\"Bond type between H1 and O: {BOND_TYPES[h_to_o_1]}\")\n",
    "print(f\"Bond type between H2 and O: {BOND_TYPES[h_to_o_2]}\")\n",
    "\n",
    "# check no bond between H and H\n",
    "# looks like the model is predicting a single bond between H1 and H2 :(\n",
    "h_to_h = edge_probs[0, 1, 2].argmax().item()\n",
    "print(f\"Bond type between H1 and H2: {BOND_TYPES[h_to_h]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth adjacency matrix for water:\n",
      "Edges:\n",
      "  Atom 0 - Atom 1: single\n",
      "  Atom 0 - Atom 2: single\n",
      "\n",
      "Overfitting model to water molecule...\n",
      "Epoch 000 | Train Loss: 0.6018 | Val Loss: 0.4967\n",
      "Epoch 001 | Train Loss: 0.5026 | Val Loss: 0.4254\n",
      "Epoch 002 | Train Loss: 0.4168 | Val Loss: 0.3621\n",
      "Epoch 003 | Train Loss: 0.3712 | Val Loss: 0.3225\n",
      "Epoch 004 | Train Loss: 0.3278 | Val Loss: 0.2888\n",
      "Epoch 005 | Train Loss: 0.2918 | Val Loss: 0.2629\n",
      "Epoch 006 | Train Loss: 0.2591 | Val Loss: 0.2288\n",
      "Epoch 007 | Train Loss: 0.2308 | Val Loss: 0.2001\n",
      "Epoch 008 | Train Loss: 0.1983 | Val Loss: 0.1697\n",
      "Epoch 009 | Train Loss: 0.1727 | Val Loss: 0.1440\n",
      "Epoch 010 | Train Loss: 0.1426 | Val Loss: 0.1137\n",
      "Epoch 011 | Train Loss: 0.1156 | Val Loss: 0.0925\n",
      "Epoch 012 | Train Loss: 0.0942 | Val Loss: 0.0711\n",
      "Epoch 013 | Train Loss: 0.0716 | Val Loss: 0.0522\n",
      "Epoch 014 | Train Loss: 0.0523 | Val Loss: 0.0378\n",
      "Epoch 015 | Train Loss: 0.0383 | Val Loss: 0.0267\n",
      "Epoch 016 | Train Loss: 0.0271 | Val Loss: 0.0180\n",
      "Epoch 017 | Train Loss: 0.0179 | Val Loss: 0.0120\n",
      "Epoch 018 | Train Loss: 0.0121 | Val Loss: 0.0079\n",
      "Epoch 019 | Train Loss: 0.0081 | Val Loss: 0.0055\n",
      "Epoch 020 | Train Loss: 0.0054 | Val Loss: 0.0035\n",
      "Epoch 021 | Train Loss: 0.0034 | Val Loss: 0.0024\n",
      "Epoch 022 | Train Loss: 0.0023 | Val Loss: 0.0015\n",
      "Epoch 023 | Train Loss: 0.0016 | Val Loss: 0.0011\n",
      "Epoch 024 | Train Loss: 0.0010 | Val Loss: 0.0007\n",
      "Epoch 025 | Train Loss: 0.0007 | Val Loss: 0.0005\n",
      "Epoch 026 | Train Loss: 0.0005 | Val Loss: 0.0004\n",
      "Epoch 027 | Train Loss: 0.0003 | Val Loss: 0.0003\n",
      "Epoch 028 | Train Loss: 0.0003 | Val Loss: 0.0002\n",
      "Epoch 029 | Train Loss: 0.0002 | Val Loss: 0.0001\n",
      "Epoch 030 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Epoch 031 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Epoch 032 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Epoch 033 | Train Loss: 0.0001 | Val Loss: 0.0001\n",
      "Epoch 034 | Train Loss: 0.0001 | Val Loss: 0.0000\n",
      "Epoch 035 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 036 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 037 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 038 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 039 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 040 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 041 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 042 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 043 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 044 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 045 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 046 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 047 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 048 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 049 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 050 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 051 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 052 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 053 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 054 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 055 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 056 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 057 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 058 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 059 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 060 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 061 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 062 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 063 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 064 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 065 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 066 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 067 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 068 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 069 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 070 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 071 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 072 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 073 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 074 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 075 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 076 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 077 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 078 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 079 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 080 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 081 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 082 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 083 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 084 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 085 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 086 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 087 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 088 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 089 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 090 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 091 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 092 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 093 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 094 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 095 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 096 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 097 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 098 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 099 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 100 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 101 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 102 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 103 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 104 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 105 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 106 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 107 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 108 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 109 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 110 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 111 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 112 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 113 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 114 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 115 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 116 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 117 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 118 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 119 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 120 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 121 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 122 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 123 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 124 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 125 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 126 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 127 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 128 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 129 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 130 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 131 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 132 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 133 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 134 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 135 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 136 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 137 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 138 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 139 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 140 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 141 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 142 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 143 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 144 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 145 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 146 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 147 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 148 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 149 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 150 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 151 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 152 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 153 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 154 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 155 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 156 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 157 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 158 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 159 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 160 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 161 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 162 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 163 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 164 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 165 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 166 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 167 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 168 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 169 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 170 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 171 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 172 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 173 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 174 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 175 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 176 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 177 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 178 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 179 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 180 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 181 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 182 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 183 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 184 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 185 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 186 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 187 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 188 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 189 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 190 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 191 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 192 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 193 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 194 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 195 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 196 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 197 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 198 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Epoch 199 | Train Loss: 0.0000 | Val Loss: 0.0000\n",
      "Training finished\n",
      "\n",
      "Bond predictions after overfitting:\n",
      "Bond type between O and H1: single\n",
      "Bond type between O and H2: single\n",
      "Bond type between H1 and H2: no bond\n",
      "Checking for spurious bonding to non-atom nodes...\n",
      "\n",
      "Probabilities:\n",
      "O-H1: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "O-H2: [1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "H1-H2: [0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "\n",
      "Predicted water molecule structure:\n",
      "          O         H1        H2      \n",
      "O         -         single    single  \n",
      "H1        single    -                 \n",
      "H2        single              -       \n"
     ]
    }
   ],
   "source": [
    "# sanity check: intensely train the model on *only* the water molecule\n",
    "# this should make the model overfit to it and predict the correct bond types\n",
    "# if it does that, we know the problem is in the model\n",
    "# the most likely culprit is the overwhelming number of \"no atom\" nodes and\n",
    "# \"no bond\" edges\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a dataloader with just the water molecule (repeating it to match API)\n",
    "def create_water_loader(water):\n",
    "    from torch_geometric.data import Batch\n",
    "    # Create a batch with just the water molecule repeated\n",
    "    water_batch = Batch.from_data_list([water] * 4)  # Batch size of 4\n",
    "\n",
    "    # Create a simple loader that just returns this batch\n",
    "    class WaterLoader:\n",
    "        def __iter__(self):\n",
    "            yield water_batch\n",
    "        def __len__(self):\n",
    "            return 1\n",
    "\n",
    "    return WaterLoader()\n",
    "\n",
    "water_loader = create_water_loader(water)\n",
    "\n",
    "# Get the ground truth adjacency matrix\n",
    "def get_ground_truth_bonds(molecule, natom):\n",
    "    print(\"Ground truth adjacency matrix for water:\")\n",
    "    edges = molecule.edge_index.cpu().numpy()\n",
    "    attrs = molecule.edge_attr.cpu().numpy()\n",
    "\n",
    "    # Print in human-readable format\n",
    "    print(\"Edges:\")\n",
    "    for i in range(min(10, edges.shape[1])):\n",
    "        src, dst = edges[0, i], edges[1, i]\n",
    "        if src < natom and dst < natom:  # Only real atoms\n",
    "            attr_idx = attrs[i].argmax()\n",
    "            bond_type = BOND_TYPES[attr_idx]\n",
    "            print(f\"  Atom {src} - Atom {dst}: {bond_type}\")\n",
    "\n",
    "    return edges, attrs\n",
    "\n",
    "get_ground_truth_bonds(water, 3) # Water has 3 atoms: O, H, H\n",
    "\n",
    "print(\"\\nOverfitting model to water molecule...\")\n",
    "water_model = copy.deepcopy(model)\n",
    "training.train_model(\n",
    "    water_model,\n",
    "    water_loader,\n",
    "    water_loader,\n",
    "    device,\n",
    "    n_epochs=200,\n",
    "    patience=1000           # prevent early stopping\n",
    ")\n",
    "\n",
    "# Check predictions from overfitted model\n",
    "with torch.no_grad():\n",
    "    edge_logits, mu, logvar, property_pred = water_model(water)\n",
    "    edge_probs = F.softmax(edge_logits, dim=-1)\n",
    "\n",
    "print(\"\\nBond predictions after overfitting:\")\n",
    "print(f\"Bond type between O and H1: {BOND_TYPES[edge_probs[0, 0, 1].argmax().item()]}\")\n",
    "print(f\"Bond type between O and H2: {BOND_TYPES[edge_probs[0, 0, 2].argmax().item()]}\")\n",
    "print(f\"Bond type between H1 and H2: {BOND_TYPES[edge_probs[0, 1, 2].argmax().item()]}\")\n",
    "print(\"Checking for spurious bonding to non-atom nodes...\")\n",
    "for i in range(3, water.x.shape[0]):\n",
    "    for j in range(water.x.shape[0]):\n",
    "        # should always be [0., 0., 0., 0., 0., 1.]\n",
    "        most_likely_type = edge_probs[0, i, j].argmax().item()\n",
    "        if most_likely_type != 4: # no bond\n",
    "            print(f\"Unexpected bond type ({i}-{j}): {BOND_TYPES[most_likely_type]}\")\n",
    "\n",
    "# Show probabilities for key bonds\n",
    "print(\"\\nProbabilities:\")\n",
    "print(f\"O-H1: {[round(x.item(), 4) for x in edge_probs[0, 0, 1]]}\")\n",
    "print(f\"O-H2: {[round(x.item(), 4) for x in edge_probs[0, 0, 2]]}\")\n",
    "print(f\"H1-H2: {[round(x.item(), 4) for x in edge_probs[0, 1, 2]]}\")\n",
    "\n",
    "# Simple visualization of predictions vs ground truth\n",
    "def compare_water_bonds(edge_probs):\n",
    "    \"\"\"Compare predicted bonds against water's known structure\"\"\"\n",
    "    structure = [\n",
    "        [\"\", \"O\", \"H1\", \"H2\"],\n",
    "        [\"O\", \"-\", \"\", \"\"],\n",
    "        [\"H1\", \"\", \"-\", \"\"],\n",
    "        [\"H2\", \"\", \"\", \"-\"]\n",
    "    ]\n",
    "\n",
    "    # Fill in predicted bonds\n",
    "    for i in range(1, 4):\n",
    "        for j in range(1, 4):\n",
    "            if i != j:\n",
    "                src, dst = i-1, j-1\n",
    "                bond_type = BOND_TYPES[edge_probs[0, src, dst].argmax().item()]\n",
    "                if bond_type != \"no bond\":\n",
    "                    structure[i][j] = bond_type\n",
    "\n",
    "    # Print table\n",
    "    print(\"\\nPredicted water molecule structure:\")\n",
    "    for row in structure:\n",
    "        print(\"  \".join(f\"{cell:8s}\" for cell in row))\n",
    "\n",
    "compare_water_bonds(edge_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
